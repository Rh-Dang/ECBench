<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ECBench: Can Multi-modal Foundation Models Understand the Egocentric World?  A Holistic Embodied Cognition Benchmark">
  <meta name="keywords" content="Benchmark, Embodied Cognition, Multi-modal Foundation Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ECBench: A Holistic Embodied Cognition Benchmark</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=3MGeRMUAAAAJ">Ronghao Dang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=7D7QL9MAAAAJ">Yuqian Yuan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=C6Bmfw8AAAAJ">Wenqi Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=Djju0V4AAAAJ">Yifei Xin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=HE-PXScAAAAJ">Boqiang Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=6CDjuw4AAAAJ">Long Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=AW2gZ8cAAAAJ">Liuyi Wang</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a>Qinyang Zeng</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=syD9lxQAAAAJ">Xin Li</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=en&user=_oYzrzAAAAAJ">Lidong Bing</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Alibaba DAMO Academy,</span>
            <span class="author-block"><sup>2</sup>Zhejiang University,</span>
            <span class="author-block"><sup>3</sup>Tongji University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.05031"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.05031"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Rh-Dang/ECBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/DAMOEC/ECBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/intro.png" alt="introduction" width="1000" height="600" />
      <h2 class="subtitle has-text-centered">
        There are 386 RGB-D videos, 4,324 QA pairs, and 30 distinct embodied cognitive abilities,
        spanning across various aspects such as perception, reasoning, self-awareness, 
        dynamic capturing, and hallucination. ECEval employs distinct evaluation methods for different 
        types of answers.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Spatial Relationship.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Mathematical Reasoning.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Trajectory Review.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/movement_imagery.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/State Dynamic.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Co-occurrence Countersense.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The enhancement of generalization in robots by large vision-language models (LVLMs)
            is increasingly evident. Therefore, the embodied cognitive abilities of LVLMs based on
            egocentric videos are of great interest. However, current datasets for embodied video
            question answering lack comprehensive and systematic evaluation frameworks.
            Critical embodied cognitive issues, such as robotic self-cognition, dynamic scene perception,
            and hallucination, are rarely addressed.
          </p>
          <p>
            To tackle these challenges, we propose ECBench, a high-quality benchmark designed to
            systematically evaluate the embodied cognitive abilities of LVLMs.
            ECBench features a diverse range of scene video sources, open and varied question formats,
            and 30 dimensions of embodied cognition. To ensure quality, balance, and high visual dependence,
            ECBench uses class-independent meticulous human annotation and multi-round question screening
            strategies. Additionally, we introduce ECEval, a comprehensive evaluation system that ensures
            the fairness and rationality of the indicators.
          </p>
          <p>
            Utilizing ECBench, we conduct extensive evaluations of proprietary, open-source,
            and task-specific LVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities
            of LVLMs, laying a solid foundation for developing reliable core models for embodied agents.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class = "title is-2">
      <span style="vertical-align: middle">Comparision of ECBench and Other Embodied / General VideoQA Benchmarks 
      </span>
    </h1>
  </div>
</section>

<section class="section" id="Dataset">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">

          <div class="content has-text-justified">
          <p>
          The current evaluation of LVLMs in embodied scenarios has the following limitations:
          <p><strong>1. Not Systematic:</strong></p>
          <p>Current benchmarks focus on independent embodied abilities, such as object recognition and counting. They lack a comprehensive top-down analysis of embodied cognition requirements, leading to limitations in evaluation hierarchy and dimensions.</p>
      
          <p><strong>2. Lack of Robot-Centric:</strong></p>
          <p>Robots often need to address questions related to their own embodiment, such as the distance to a target, or their historical trajectory. However, benchmarks like OpenEQA focus solely on third-person scenario questions, significantly overlooking the evaluation of  robots' self-awareness.</p>
          
          <p><strong>3. Lack of Dynamics:</strong></p>
          <p>In the real physical world, scene dynamics are perpetually ongoing.  For complex tasks like ``Revert the screen content to before you faced the whiteboard," a robot must recognize these dynamics and accurately recall their timing and process. However, current embodied question answering benchmarks typically overlook these dynamic aspects, defaulting to static context assumptions. </p>
          
          <p><strong>4. Hallucination issue:</strong></p>
          <P> Although the hallucination phenomenon has been extensively analyzed within LVLMs, embodied-based question answering presents unique hallucination challenges. For instance, LVLMs like GPT-4o often rely too much on common sense in the scene when answering questions in counterintuitive scenes, resulting in incorrect answers. These embodied hallucination issues remain unexplored in the academic literature. </P>
          </p>

          <centering>
            <div class="column is-six-fifths" width="100%">
              <div style="text-align: left;">
                  <img id="pie" width="100%" src="./static/images/compare_table.png">
                  <p style="font-family:Times New Roman">
                    <font size=4>
                      <b>Comparing ECBench and widely adopted Embodied / General VideoQA benchmarks.</b>
                      ECBench has significant advantages in terms of quality, diversity and evaluation dimensions.
                    </font>
                </div>
              </div>
            
              <div class="column is-six-fifths" width="100%">
                <div style="text-align: left;">
                    <img id="pie" width="100%" src="./static/images/data_analysis.png">
                    <p style="font-family:Times New Roman">
                      <font size=4>
                        <b>Data analysis of ECBench</b>
                        reflects a rich diversity of scenario categories, video sources, and evaluation dimensions.
                      </font>
                  </div>
                </div>

          </centering>
          <br>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class = "title is-2">
      <span style="vertical-align: middle"> Capability Taxonomy of ECBench
      </span>
    </h1>
  </div>
</section>

<section class="section" id="Captioner">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <centering>
            <div style="text-align: center;"><img id="captioner" width="100%" src="./static/images/Capability_Taxonomy.png"></div>
            <div style="text-align: left;">
              <p style="font-family:Times New Roman">
                <font size=4>
                  <b> Overview of embodied cognition dimensions in ECBench.</b>
                  ECBench includes three subsets: static scenes, dynamic scenes, 
                  and hallucination, evaluating a total of 30 embodied cognitive abilities.
                </font>
              </p>
            </div>
          </centering>
          <br>

                
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class = "title is-2">
      <span style="vertical-align: middle"> ECBench Leaderboard
      </span>
    </h1>
  </div>
</section>


<section class="section" id="Leaderboard">
  <div class="container"> 
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <div class="content">
          <p>Static Scene: SB (Scene-Based), RC (Robot-Centric)</p>
          <p>Dynamic Scene: ID (Information Dynamics), QD (Quantity Dynamics), SPD (Spatial Dynamics), STD (State Dynamics)</p>
          <p>Hallucination: UI (Over-Confidence in User Input), CS (Over-Confidence in Common Sense)</p>
          <p>The <u><b>best</b></u> results are highlighted in <u><b>bold and underlined</b></u>.
          <!-- <p>You can also view the latest <u><b>ECBench</b></u> Leaderboard at <a href="https://huggingface.co/spaces/opencompass/openvlm_video_leaderboard">OpenVLM Video Leaderboard</a>! -->

          <!-- <script
            type="module"
            src="https://gradio.s3-us-west-2.amazonaws.com/4.44.0/gradio.js"
          ></script>

          <gradio-app src="https://opencompass-openvlm-video-leaderboard.hf.space"></gradio-app> -->
          <table class="table is-striped js-sort-table" id="results">
            <thead>
              <tr>
                <th rowspan="2" style="vertical-align: middle;"><strong>Model</strong></th>
                <th colspan="3" style="text-align: center;"><strong>Static Scene</strong></th>
                <th colspan="5" style="text-align: center;"><strong>Dynamic Scene</strong></th>
                <th colspan="3" style="text-align: center;"><strong>Hallucination</strong></th>
                <th rowspan="2" style="vertical-align: middle;"><strong>Overall Mean</strong></th>
                <th rowspan="2" style="vertical-align: middle;"><strong>Model Type</strong></th>
              </tr>
              <tr>
                <th style="text-align: center;"><strong>SB</strong></th>
                <th style="text-align: center;"><strong>RC</strong></th>
                <th style="text-align: center;"><strong>Mean</strong></th>
                <th style="text-align: center;"><strong>ID</strong></th>
                <th style="text-align: center;"><strong>QD</strong></th>
                <th style="text-align: center;"><strong>SPD</strong></th>
                <th style="text-align: center;"><strong>STD</strong></th>
                <th style="text-align: center;"><strong>Mean</strong></th>
                <th style="text-align: center;"><strong>UI</strong></th>
                <th style="text-align: center;"><strong>CS</strong></th>
                <th style="text-align: center;"><strong>Mean</strong></th>
              </tr>
            </thead>
            <tbody>

              <tr>
                <td>GPT-4o-[32f] ðŸ¥‡</td>
                <td><u><b>59.74</b></u></td>
                <td><u><b>49.04</b></u></td>
                <td><u><b>55.06</b></u></td>
                <td>25.71</td>
                <td>14.29</td>
                <td><u><b>22.73</b></u></td>
                <td>24.04</td>
                <td>22.74</td>
                <td>7.69</td>
                <td><u><b>57.01</b></u></td>
                <td><u><b>31.80</b></u></td>
                <td><u><b>50.35</b></u></td>
                <td>Proprietary Image-LVLMs</td>
              </tr>

              <tr>
                <td>Qwen2VL-72B-[20f] ðŸ¥ˆ</td>
                <td>52.40</td>
                <td>43.95</td>
                <td>49.19</td>
                <td><u><b>37.14</b></u></td>
                <td>11.43</td>
                <td>18.18</td>
                <td><u><b>24.05</b></u></td>
                <td><u><b>24.03</b></u></td>
                <td>5.86</td>
                <td>42.38</td>
                <td>23.71</td>
                <td>44.62</td>
                <td>Open-Source Image-LVLMs</td>
              </tr>

              <tr>
                <td>GPT-4o-mini-[32f] ðŸ¥‰</td>
                <td>51.20</td>
                <td>43.12</td>
                <td>48.13</td>
                <td>26.35</td>
                <td><u><b>20.00</b></u></td>
                <td>15.15</td>
                <td>18.10</td>
                <td>19.68</td>
                <td><u><b>9.89</b></u></td>
                <td>41.38</td>
                <td>25.28</td>
                <td>43.69</td>
                <td>Proprietary Image-LVLMs</td>
              </tr>

              <tr>
                <td>LongVA-7B-[384f] ðŸ¥‰</td>
                <td>49.03</td>
                <td>38.56</td>
                <td>45.05</td>
                <td>28.57</td>
                <td>8.57</td>
                <td>13.94</td>
                <td>16.67</td>
                <td>17.82</td>
                <td>8.06</td>
                <td>33.49</td>
                <td>20.49</td>
                <td>40.47</td>
                <td>Native Video-LVLMs</td>
              </tr>

              <tr>
                <td>Qwen2VL-7B-[20f]</td>
                <td>46.21</td>
                <td>39.33</td>
                <td>43.60</td>
                <td>32.70</td>
                <td>8.57</td>
                <td>19.70</td>
                <td>18.33</td>
                <td>20.97</td>
                <td>4.40</td>
                <td>39.08</td>
                <td>21.35</td>
                <td>39.57</td>
                <td>Open-Source Image-LVLMs</td>
              </tr>

              <tr>
                <td>GPT-4v-[8f]</td>
                <td>45.09</td>
                <td>40.16</td>
                <td>43.22</td>
                <td>29.52</td>
                <td>8.57</td>
                <td>22.12</td>
                <td>20.24</td>
                <td>21.45</td>
                <td>9.16</td>
                <td>32.11</td>
                <td>20.37</td>
                <td>39.16</td>
                <td>Proprietary Image-LVLMs</td>
              </tr>

              <tr>
                <td>Kangaroo-8B-[64f]</td>
                <td>40.79</td>
                <td>40.01</td>
                <td>40.49</td>
                <td>19.20</td>
                <td>17.14</td>
                <td>16.97</td>
                <td>15.95</td>
                <td>17.42</td>
                <td>0.37</td>
                <td>33.49</td>
                <td>16.55</td>
                <td>36.23</td>
                <td>Native Video-LVLMs</td>
              </tr>

              <tr>
                <td>InternVL2-40B-[20f]</td>
                <td>41.27</td>
                <td>38.09</td>
                <td>40.06</td>
                <td>33.33</td>
                <td>0.00</td>
                <td>12.12</td>
                <td>21.66</td>
                <td>19.03</td>
                <td>3.66</td>
                <td>29.58</td>
                <td>16.33</td>
                <td>35.94</td>
                <td>Open-Source Image-LVLMs</td>
              </tr>

              <tr>
                <td>Video-LLaVA-7B-[8f]</td>
                <td>41.21</td>
                <td>37.25</td>
                <td>39.71</td>
                <td>18.41</td>
                <td>14.29</td>
                <td>19.20</td>
                <td>16.67</td>
                <td>17.66</td>
                <td>1.10</td>
                <td>26.97</td>
                <td>13.75</td>
                <td>35.25</td>
                <td>Native Video-LVLMs</td>
              </tr>

              <tr>
                <td>AlanaVLM-7B-[64f]</td>
                <td>40.38</td>
                <td>36.61</td>
                <td>38.95</td>
                <td>19.05</td>
                <td>5.71</td>
                <td>19.70</td>
                <td>14.76</td>
                <td>15.89</td>
                <td>1.83</td>
                <td>29.96</td>
                <td>15.58</td>
                <td>34.75</td>
                <td>Embodied / Egocentric LVLMs</td>
              </tr>

              <tr>
                <td>Video-LLaMA2-7B-[16f]</td>
                <td>41.23</td>
                <td>33.52</td>
                <td>38.30</td>
                <td>16.51</td>
                <td>5.71</td>
                <td>16.97</td>
                <td>11.19</td>
                <td>13.31</td>
                <td>4.03</td>
                <td>24.29</td>
                <td>13.93</td>
                <td>33.87</td>
                <td>Native Video-LVLMs</td>
              </tr>

              <tr>
                <td>Idefics3-8B-[20f]</td>
                <td>36.56</td>
                <td>38.09</td>
                <td>37.14</td>
                <td>18.10</td>
                <td>8.57</td>
                <td>10.61</td>
                <td>19.29</td>
                <td>15.16</td>
                <td>4.76</td>
                <td>28.89</td>
                <td>16.55</td>
                <td>33.35</td>
                <td>Open-Source Image-LVLMs</td>
              </tr>

              <tr>
                <td>GeLM-7B-[180f]</td>
                <td>25.72</td>
                <td>23.70</td>
                <td>24.95</td>
                <td>5.08</td>
                <td>5.71</td>
                <td>8.18</td>
                <td>3.51</td>
                <td>5.48</td>
                <td>0.37</td>
                <td>12.49</td>
                <td>6.29</td>
                <td>21.54</td>
                <td>Embodied / Egocentric LVLMs</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ECBench,
      title={ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark},
      author={Dang, Ronghao and Yuan, Yuqian and Zhang, Wenqi and Xin, Yifei and Zhang, Boqiang and Li, Long and Wang, Liuyi and Zeng, Qinyang and Li, Xin and Bing, Lidong},
      journal={arXiv preprint arXiv:2501.05031},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2501.05031">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Rh-Dang/ECBench" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/Rh-Dang/ECBench">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
